{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28a340c3-ea17-492a-a4ec-c77bf08d02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import collections\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torchtext.vocab import vocab\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import get_tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e1f11d-96bf-4cfe-8fd5-2e931e56f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cae36d3-aa54-47f5-89d6-80d0beb5c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main training script for Beer and Hotel.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "\n",
    "from rrtl.utils import (\n",
    "    get_model_class,\n",
    "    get_optimizer_class,\n",
    "    args_factory,\n",
    "    save_args,\n",
    "    save_ckpt,\n",
    "    load_ckpt,\n",
    "    build_vib_path\n",
    ")\n",
    "\n",
    "from rrtl.logging_utils import log,log_cap\n",
    "from rrtl.visualize import visualize_rationale,visualize_prob\n",
    "from rrtl.stats import gold_rationale_capture_rate,cal_pns,first_capture\n",
    "from rrtl.config import Config\n",
    "import warnings\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32e545bb-325c-4601-a8f8-6a1af0e74bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_class(args):\n",
    "    if args.dataset_name in ('beer'):#, 'squad-addonesent', 'squad-addsent', 'squad-addonesent-pos0'):\n",
    "        dataloader_class = SentimentDataLoader\n",
    "    elif args.dataset_name == 'ga':\n",
    "          dataloader_class = GADataLoader\n",
    "    #    dataloader_class = SQUADNegRationaleDataLoader\n",
    "    elif args.method=='a2r':\n",
    "        dataloader_class = SentimentDataLoader\n",
    "    else:\n",
    "        raise ValueError('Dataloader not implemented.')\n",
    "    return dataloader_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7887a87d-0be1-4725-b12a-d3969c320c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_token_map(encoder_type):\n",
    "    if encoder_type.startswith('roberta'):\n",
    "        special_token_map = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'sep_token': '</s>',\n",
    "            'cls_token': '<s>',\n",
    "            'unk_token': '<unk>',\n",
    "            'pad_token': '<pad>',\n",
    "            'mask_token': '<mask>',\n",
    "        }\n",
    "    elif encoder_type.startswith('bert') or encoder_type.startswith('distilbert'):\n",
    "        special_token_map = {\n",
    "            'sep_token': '[SEP]',\n",
    "            'cls_token': '[CLS]',\n",
    "            'unk_token': '[UNK]',\n",
    "            'pad_token': '[PAD]',\n",
    "            'mask_token': '[MASK]',\n",
    "        }\n",
    "    return special_token_map\n",
    "\n",
    "\n",
    "class BaseDataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.tok_kwargs = config.TOK_KWARGS\n",
    "        self.tok_kwargs['max_length'] = self.args.max_length\n",
    "        if self.args.dataset_name=='ga':\n",
    "            with open('ga_code.pkl','rb') as f:\n",
    "              self.tokenizer=pickle.load(f)\n",
    "        elif self.args.encoder_type.startswith('bert') or self.args.encoder_type.startswith('distilbert'):\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', cache_dir=self.args.cache_dir)\n",
    "        elif self.args.encoder_type.startswith('roberta'):\n",
    "            self.tokenizer = RobertaTokenizerFast.from_pretrained(self.args.encoder_type, cache_dir=self.args.cache_dir)\n",
    "        \n",
    "        self.dataset_name_to_dataset_class = {\n",
    "            'beer': SentimentDataset,\n",
    "            'hotel': SentimentDataset\n",
    "        }\n",
    "        self._dataloaders = {}\n",
    "        self.special_token_map = get_special_token_map(self.args.encoder_type)\n",
    "\n",
    "    def _load_processed_data(self, mode):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_dataloader(self, data, mode):\n",
    "        dataset = self.dataset_name_to_dataset_class[self.args.dataset_name](\n",
    "            self.args,\n",
    "            data,\n",
    "            self.tokenizer,\n",
    "            self.tok_kwargs\n",
    "        )\n",
    "        collate_fn = dataset.collater\n",
    "        batch_size = self.args.batch_size\n",
    "        shuffle = True if mode == 'train' else False\n",
    "        \n",
    "        self._dataloaders[mode] = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "        print(f'[{mode}] dataloader built => {len(dataset)} examples')\n",
    "    \n",
    "    def build(self, mode):\n",
    "        data = self._load_raw_data(mode)\n",
    "        self._build_dataloader(data, mode)\n",
    "\n",
    "    def build_all(self):\n",
    "        for mode in ['train', 'dev', 'test']:\n",
    "            self.build(mode)\n",
    "    \n",
    "    def __getitem__(self, mode):\n",
    "        return self._dataloaders[mode]\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._dataloaders['train']\n",
    "\n",
    "    @property\n",
    "    def dev(self):\n",
    "        return self._dataloaders['dev']\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return self._dataloaders['test']\n",
    "\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, args, data, tokenizer, tok_kwargs):\n",
    "        self.args = args\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tok_kwargs = tok_kwargs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    @property\n",
    "    def num_batches(self):\n",
    "        return len(self.data) // self.args.batch_size\n",
    "\n",
    "\n",
    "class SentimentDataLoader(BaseDataLoader):\n",
    "    def __init__(self, args):\n",
    "        super(SentimentDataLoader, self).__init__(args)\n",
    "        if args.dataset_split == 'all':\n",
    "            self.build_all()\n",
    "        else:\n",
    "            self.build(args.dataset_split)\n",
    "\n",
    "    def _load_raw_data(self, mode):\n",
    "            datapoints = []\n",
    "            aspect=self.args.aspect\n",
    "            #scale='normal'\n",
    "            scale=self.args.scale\n",
    "            print('aspect:',aspect)\n",
    "            print('mode:',mode)\n",
    "            print('scale:',scale)\n",
    "            \n",
    "            #scale='noise'\n",
    "            \n",
    "            if mode=='pns':\n",
    "              path = config.DATA_DIR / f'sentiment/data/pns'\n",
    "            else:\n",
    "              if scale=='normal':\n",
    "                if self.args.attack_path is not None:\n",
    "                    path = self.args.attack_path\n",
    "                elif self.args.dataset_name == 'beer' and mode in ('train', 'dev'):\n",
    "                    path = config.DATA_DIR / f'sentiment/data/source/beer_{aspect}.{mode}'\n",
    "                elif self.args.dataset_name == 'beer' and mode == 'test':\n",
    "                    path = config.DATA_DIR / f'sentiment/data/target/beer_{aspect}.train'\n",
    "                elif self.args.dataset_name == 'hotel' and mode in ('train', 'dev'):\n",
    "                    path = config.DATA_DIR / f'sentiment/data/oracle/hotel_{aspect}.{mode}'\n",
    "                elif self.args.dataset_name == 'hotel' and mode == 'test':\n",
    "                    path = config.DATA_DIR / f'sentiment/data/target/hotel_{aspect}.train'\n",
    "                else:\n",
    "                    raise ValueError('Dataset name not supported.')\n",
    "                    \n",
    "              if scale=='small':\n",
    "                if self.args.attack_path is not None:\n",
    "                    path = self.args.attack_path\n",
    "                elif self.args.dataset_name == 'beer' and mode in ('train', 'dev'):\n",
    "                    path = config.DATA_DIR / f'sentiment/data/source/beer_{aspect}.{mode}_120'\n",
    "                elif self.args.dataset_name == 'beer' and mode == 'test':\n",
    "                    path = config.DATA_DIR / f'sentiment/data/target/beer_{aspect}.train_120'\n",
    "                elif self.args.dataset_name == 'hotel' and mode in ('train', 'dev'):\n",
    "                    path = config.DATA_DIR / f'sentiment/data/oracle/hotel_{aspect}.{mode}_120'\n",
    "                elif self.args.dataset_name == 'hotel' and mode == 'test':\n",
    "                    path = config.DATA_DIR / f'sentiment/data/target/hotel_{aspect}.train_120'\n",
    "                else:\n",
    "                    raise ValueError('Dataset name not supported.')\n",
    "                    \n",
    "              if scale=='noise':\n",
    "                if self.args.dataset_name == 'beer' and mode in ('train', 'dev'):\n",
    "                    path = config.DATA_DIR / f'sentiment/data/source/beer_{aspect}.{mode}_noise'\n",
    "                elif self.args.dataset_name == 'beer' and mode == 'test':\n",
    "                    path = config.DATA_DIR / f'sentiment/data/target/beer_{aspect}.train'\n",
    "            \n",
    "                    \n",
    "              if scale=='causal':\n",
    "                if self.args.dataset_name == 'beer' and mode in ('train', 'dev'):\n",
    "                    path = config.DATA_DIR / f'sentiment/data/source/beer_{aspect}.{mode}_causal'\n",
    "                elif self.args.dataset_name == 'beer' and mode == 'test':\n",
    "                    path = config.DATA_DIR / f'sentiment/data/target/beer_{aspect}.train_causal' \n",
    "              \n",
    "                \n",
    "            df = pd.read_csv(path, delimiter='\\t')\n",
    "            for index, row in df.iterrows():\n",
    "                label = row['label']\n",
    "\n",
    "                # this could be applied to both beer and hotel\n",
    "                if label >= 0.6:\n",
    "                    label = 1  # pos\n",
    "                elif label <= 0.4:\n",
    "                    label = 0  # neg\n",
    "                else:\n",
    "                    continue\n",
    "                text = row['text']\n",
    "                if 'rationale' in row:\n",
    "                    rationale = [int(r) for r in row['rationale'].split()]\n",
    "                else:\n",
    "                    rationale = [-1] * len(row['text'].split())\n",
    "                datapoints.append({\n",
    "                    'label': label,\n",
    "                    'text': text,\n",
    "                    'rationale': rationale,\n",
    "                })\n",
    "            if self.args.debug:\n",
    "              datapoints = datapoints[:200]\n",
    "            return datapoints\n",
    "\n",
    "    def _load_processed_data(self, mode):\n",
    "        processed_datapoints = []\n",
    "        datapoints = self._load_raw_data(mode)\n",
    "        for datapoint in tqdm(datapoints, total=len(datapoints)):\n",
    "            label = datapoint['label']\n",
    "            input_tokens = ['[CLS]'] + datapoint['text'].split()\n",
    "            rationale = [0] + datapoint['rationale']\n",
    "            input_ids = []\n",
    "            attention_mask = []\n",
    "            rationale_ = []\n",
    "            for input_token, r in zip(input_tokens, rationale):\n",
    "                tokenized = self.tokenizer.encode_plus(input_token, add_special_tokens=False)\n",
    "                input_ids += tokenized['input_ids']\n",
    "                attention_mask += tokenized['attention_mask']\n",
    "                ## make rationale cover subword\n",
    "                rationale_ += [r] * len(tokenized['input_ids'])\n",
    "            #check length of sub-word toke\n",
    "            #print(len(input_ids))\n",
    "            if len(input_ids) >= self.args.max_length:\n",
    "                input_ids = input_ids[:self.args.max_length - 1] + [102]\n",
    "                attention_mask = attention_mask[:self.args.max_length - 1] + [1]\n",
    "                rationale = rationale_[:self.args.max_length - 1] + [0]\n",
    "            else:\n",
    "                input_ids = input_ids + [102] #102 is [SEP]\n",
    "                attention_mask = attention_mask + [1]\n",
    "                rationale = rationale_ + [0]\n",
    "                \n",
    "            input_ids = self.pad(input_ids)\n",
    "            attention_mask = self.pad(attention_mask)\n",
    "            rationale = self.pad(rationale)\n",
    "\n",
    "            assert len(input_ids) == self.args.max_length\n",
    "\n",
    "            processed_datapoints.append({\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'label': label,\n",
    "                'rationale': rationale,\n",
    "            })\n",
    "        return processed_datapoints\n",
    "\n",
    "    def pad(self, seq):\n",
    "        return seq + (self.args.max_length - len(seq)) * [0]\n",
    "\n",
    "\n",
    "class SentimentDataset(BaseDataset):\n",
    "    def __init__(self, args, data, tokenizer, tok_kwargs):\n",
    "        super(SentimentDataset, self).__init__(args, data, tokenizer, tok_kwargs)\n",
    "\n",
    "    def collater(self, batch):\n",
    "        device = 'cuda' if self.args.use_cuda else 'cpu'\n",
    "        return [datapoint for datapoint in batch]\n",
    "\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2685a1a-a1c8-430b-b43e-a7d8b3663079",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "        parser = argparse.ArgumentParser()\n",
    "        \n",
    "\n",
    "        # experiment\n",
    "        parser.add_argument(\"--scale\", type=str, default=\"normal\", help=\"[small |normal]\")\n",
    "        parser.add_argument(\"--dataset-split\", type=str, default=\"all\", help=\"[all | train | dev | test]\")\n",
    "        parser.add_argument(\"--encoder-type\", type=str, default=\"bert-base-uncased\")\n",
    "        parser.add_argument(\"--decoder-type\", type=str, default=\"bert-base-uncased\")\n",
    "        parser.add_argument(\"--wandb\", action=\"store_true\")\n",
    "        parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "        parser.add_argument(\"--cache_dir\", type=str, default=config.CACHE_DIR)\n",
    "        parser.add_argument(\"--overwrite_cache\", action=\"store_true\")\n",
    "        parser.add_argument(\"--attack_path\", type=str, default=None)\n",
    "\n",
    "        # cuda\n",
    "        parser.add_argument(\"--device_id\", type=int, default=0)\n",
    "        parser.add_argument(\"--dataparallel\", action=\"store_true\")\n",
    "        parser.add_argument(\"--inspect-gpu\", action=\"store_true\")\n",
    "        parser.add_argument(\"--disable-cuda\", action=\"store_true\")\n",
    "\n",
    "        # printing, logging, and checkpointing\n",
    "        parser.add_argument(\"--print-every\", type=int, default=80)\n",
    "        parser.add_argument(\"--eval-interval\", type=int, default=500)\n",
    "        parser.add_argument(\"--disable-ckpt\", action=\"store_true\")\n",
    "\n",
    "        # training\n",
    "        parser.add_argument(\"--train_opt\", type=int, default=0)\n",
    "        parser.add_argument(\"--warm_epoch\", type=int, default=0)\n",
    "        parser.add_argument(\"--seed\", type=int, default=42)\n",
    "        parser.add_argument(\"--nseeds\", type=int, default=5)\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "        parser.add_argument(\"--max_length\", type=int, default=300)\n",
    "        parser.add_argument(\"--num_epoch\", type=int, default=20)\n",
    "        parser.add_argument(\"--lr\", type=float, default=5e-5)\n",
    "        parser.add_argument(\"--dropout_rate\", type=float, default=0.2)\n",
    "        parser.add_argument(\"--no-shuffle\", action=\"store_true\")\n",
    "        parser.add_argument(\"--optimizer\", type=str, default=\"adamw\")\n",
    "        parser.add_argument(\"--grad_accumulation_steps\", type=int, default=1)\n",
    "\n",
    "        # VIB model\n",
    "        parser.add_argument(\"--k\", type=int, default=3) # number of samples for PNS\n",
    "        parser.add_argument(\"--mu\", type=float, default=0.1) # weight for PNS\n",
    "        #parser.add_argument(\"--alpha\", type=float, default=0.0) #for concise loss\n",
    "        parser.add_argument(\"--lambda1\", type=float, default=0.0) #for concise loss\n",
    "        parser.add_argument(\"--lambda2\", type=float, default=0.0) #for continuity loss\n",
    "        parser.add_argument(\"--tau\", type=float, default=1.0)\n",
    "        parser.add_argument(\"--pi\", type=float, default=0.2)\n",
    "        parser.add_argument(\"--beta\", type=float, default=0.0)\n",
    "        parser.add_argument(\"--gamma\", type=float, default=1.0)\n",
    "        parser.add_argument(\"--gamma2\", type=float, default=1.0)\n",
    "        parser.add_argument(\"--use-gold-rationale\", action=\"store_true\")\n",
    "        parser.add_argument(\"--use-neg-rationale\", action=\"store_true\")\n",
    "        parser.add_argument(\"--fix-input\", type=str, default=None)\n",
    "\n",
    "        # SPECTRA model\n",
    "        parser.add_argument(\"--budget\", type=int, default=None)\n",
    "        parser.add_argument(\"--budget_ratio\", type=float, default=None)\n",
    "        parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
    "        parser.add_argument(\"--solver_iter\", type=int, default=100)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        args = parser.parse_args(\"\")\n",
    "        \n",
    "        args.run_name='beer_rnp' \n",
    "        args.scale='small'\n",
    "        args.dataset_name='beer' \n",
    "        args.model_type='rnp_beer_token'\n",
    "        args.aspect='Palate'\n",
    "        args.method = args.run_name.split('_')[1]\n",
    "        \n",
    "        args.method = args.run_name.split('_')[1]\n",
    "        args.use_cuda=True\n",
    "        \n",
    "        args=args_factory(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94d3c740-5b1a-4ece-96ce-51d96b9d5373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect: Palate\n",
      "mode: train\n",
      "scale: small\n",
      "[train] dataloader built => 9592 examples\n",
      "aspect: Palate\n",
      "mode: dev\n",
      "scale: small\n",
      "[dev] dataloader built => 2294 examples\n",
      "aspect: Palate\n",
      "mode: test\n",
      "scale: small\n",
      "[test] dataloader built => 200 examples\n"
     ]
    }
   ],
   "source": [
    "dataloader_class = get_dataloader_class(args)\n",
    "dl = dataloader_class(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94549d3a-5737-451a-b596-efda487fe363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SentimentDataLoader at 0x7f7dc1662910>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47237a76-ebfa-4735-a7d4-58dfbecd2511",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(dl.train):\n",
    "    sentence=batch[0]['text']\n",
    "    words=sentence.strip().split()\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c0a44b79-cada-4166-bedc-49a28a5a4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"rrtl/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cf431c32-4932-443b-aece-5fb4458b36fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ssl', 'k', 'wj', 'sasa', 'ssas']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.tokenize(\"sslk wjsasa ssas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bdc338b9-4179-4a00-a655-ed8e783ef3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sslk', 'wjsasa', 'ssas']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"sslk wjsasa ssas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6044ae48-d213-413e-b8e2-ca4079c5f0bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'wjsasa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_122900/2927741552.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mglobal_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGloVe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'840B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mglobal_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vecs_by_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sslk wjsasa ssas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_case_backup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mglobal_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sslk wjsasa ssas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'wjsasa'"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "global_vectors = GloVe(name='840B', dim=300)\n",
    "global_vectors.get_vecs_by_tokens(tokenizer(\"sslk wjsasa ssas\"), lower_case_backup=True)\n",
    "global_vectors.stoi[tokenizer(\"sslk wjsasa ssas\")[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab520e62-0ab2-4cb2-bd8d-324450745834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"clear dark red colored beer with a small tan head . smells of sweet malt and smoke , distinct toffee candy smell . slight coffee aroma but really this is more small candy maker caramel and less patent malt . starts out smoky and pleasant , tastes drier than it smells . keeps hitting the smoke and artisan caramel flavors . quite tasty and a smoked porter by taste . mouthfeel is light , blame the utah 4 % on tap only laws for brewing all their beers dry . still this is a mighty tasty `` light porter '' not really a style but an accurate description .\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ab793f1-abf0-4628-9793-7e1fab3b985e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clear',\n",
       " 'dark',\n",
       " 'red',\n",
       " 'colored',\n",
       " 'beer',\n",
       " 'with',\n",
       " 'a',\n",
       " 'small',\n",
       " 'tan',\n",
       " 'head',\n",
       " '.',\n",
       " 'smells',\n",
       " 'of',\n",
       " 'sweet',\n",
       " 'malt',\n",
       " 'and',\n",
       " 'smoke',\n",
       " ',',\n",
       " 'distinct',\n",
       " 'toffee',\n",
       " 'candy',\n",
       " 'smell',\n",
       " '.',\n",
       " 'slight',\n",
       " 'coffee',\n",
       " 'aroma',\n",
       " 'but',\n",
       " 'really',\n",
       " 'this',\n",
       " 'is',\n",
       " 'more',\n",
       " 'small',\n",
       " 'candy',\n",
       " 'maker',\n",
       " 'caramel',\n",
       " 'and',\n",
       " 'less',\n",
       " 'patent',\n",
       " 'malt',\n",
       " '.',\n",
       " 'starts',\n",
       " 'out',\n",
       " 'smoky',\n",
       " 'and',\n",
       " 'pleasant',\n",
       " ',',\n",
       " 'tastes',\n",
       " 'drier',\n",
       " 'than',\n",
       " 'it',\n",
       " 'smells',\n",
       " '.',\n",
       " 'keeps',\n",
       " 'hitting',\n",
       " 'the',\n",
       " 'smoke',\n",
       " 'and',\n",
       " 'artisan',\n",
       " 'caramel',\n",
       " 'flavors',\n",
       " '.',\n",
       " 'quite',\n",
       " 'tasty',\n",
       " 'and',\n",
       " 'a',\n",
       " 'smoked',\n",
       " 'porter',\n",
       " 'by',\n",
       " 'taste',\n",
       " '.',\n",
       " 'mouthfeel',\n",
       " 'is',\n",
       " 'light',\n",
       " ',',\n",
       " 'blame',\n",
       " 'the',\n",
       " 'utah',\n",
       " '4',\n",
       " '%',\n",
       " 'on',\n",
       " 'tap',\n",
       " 'only',\n",
       " 'laws',\n",
       " 'for',\n",
       " 'brewing',\n",
       " 'all',\n",
       " 'their',\n",
       " 'beers',\n",
       " 'dry',\n",
       " '.',\n",
       " 'still',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'mighty',\n",
       " 'tasty',\n",
       " '``',\n",
       " 'light',\n",
       " 'porter',\n",
       " \"'\",\n",
       " \"'\",\n",
       " 'not',\n",
       " 'really',\n",
       " 'a',\n",
       " 'style',\n",
       " 'but',\n",
       " 'an',\n",
       " 'accurate',\n",
       " 'description',\n",
       " '.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef458a0e-c524-4455-b5c7-21bc73d93628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1146,  0.3763, -0.4446,  ...,  0.0250,  0.3489,  0.1445],\n",
       "        [ 0.2370, -0.0127,  0.0258,  ..., -0.1554, -0.1004,  0.0244],\n",
       "        [-0.2490, -0.2322, -0.0272,  ..., -0.3526,  0.4042,  0.1854],\n",
       "        ...,\n",
       "        [-0.6502,  0.3831, -0.6818,  ..., -0.2603,  0.3001,  0.4126],\n",
       "        [-0.1262, -0.0680, -0.5887,  ..., -0.2915, -0.1349, -0.1408],\n",
       "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_vectors.get_vecs_by_tokens(tokenizer(sentence), lower_case_backup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6dc291-601e-47f0-9ee9-737896b73970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
